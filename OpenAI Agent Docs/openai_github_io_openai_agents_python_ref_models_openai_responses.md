[Skip to content](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#openai-responses-model)

# `OpenAI Responses model`

### OpenAIResponsesModel

Bases: `Model`

Implementation of `Model` that uses the OpenAI Responses API.

Source code in `src/agents/models/openai_responses.py`

|     |     |
| --- | --- |
| ```<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>``` | ```md-code__content<br>class OpenAIResponsesModel(Model):<br>    """<br>    Implementation of `Model` that uses the OpenAI Responses API.<br>    """<br>    def __init__(<br>        self,<br>        model: str | ChatModel,<br>        openai_client: AsyncOpenAI,<br>    ) -> None:<br>        self.model = model<br>        self._client = openai_client<br>    def _non_null_or_not_given(self, value: Any) -> Any:<br>        return value if value is not None else NOT_GIVEN<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> ModelResponse:<br>        with response_span(disabled=tracing.is_disabled()) as span_response:<br>            try:<br>                response = await self._fetch_response(<br>                    system_instructions,<br>                    input,<br>                    model_settings,<br>                    tools,<br>                    output_schema,<br>                    handoffs,<br>                    stream=False,<br>                )<br>                if _debug.DONT_LOG_MODEL_DATA:<br>                    logger.debug("LLM responded")<br>                else:<br>                    logger.debug(<br>                        "LLM resp:\n"<br>                        f"{json.dumps([x.model_dump() for x in response.output], indent=2)}\n"<br>                    )<br>                usage = (<br>                    Usage(<br>                        requests=1,<br>                        input_tokens=response.usage.input_tokens,<br>                        output_tokens=response.usage.output_tokens,<br>                        total_tokens=response.usage.total_tokens,<br>                    )<br>                    if response.usage<br>                    else Usage()<br>                )<br>                if tracing.include_data():<br>                    span_response.span_data.response = response<br>                    span_response.span_data.input = input<br>            except Exception as e:<br>                span_response.set_error(<br>                    SpanError(<br>                        message="Error getting response",<br>                        data={<br>                            "error": str(e) if tracing.include_data() else e.__class__.__name__,<br>                        },<br>                    )<br>                )<br>                request_id = e.request_id if isinstance(e, APIStatusError) else None<br>                logger.error(f"Error getting response: {e}. (request_id: {request_id})")<br>                raise<br>        return ModelResponse(<br>            output=response.output,<br>            usage=usage,<br>            referenceable_id=response.id,<br>        )<br>    async def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> AsyncIterator[ResponseStreamEvent]:<br>        """<br>        Yields a partial message as it is generated, as well as the usage information.<br>        """<br>        with response_span(disabled=tracing.is_disabled()) as span_response:<br>            try:<br>                stream = await self._fetch_response(<br>                    system_instructions,<br>                    input,<br>                    model_settings,<br>                    tools,<br>                    output_schema,<br>                    handoffs,<br>                    stream=True,<br>                )<br>                final_response: Response | None = None<br>                async for chunk in stream:<br>                    if isinstance(chunk, ResponseCompletedEvent):<br>                        final_response = chunk.response<br>                    yield chunk<br>                if final_response and tracing.include_data():<br>                    span_response.span_data.response = final_response<br>                    span_response.span_data.input = input<br>            except Exception as e:<br>                span_response.set_error(<br>                    SpanError(<br>                        message="Error streaming response",<br>                        data={<br>                            "error": str(e) if tracing.include_data() else e.__class__.__name__,<br>                        },<br>                    )<br>                )<br>                logger.error(f"Error streaming response: {e}")<br>                raise<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        stream: Literal[True],<br>    ) -> AsyncStream[ResponseStreamEvent]: ...<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        stream: Literal[False],<br>    ) -> Response: ...<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        stream: Literal[True] | Literal[False] = False,<br>    ) -> Response | AsyncStream[ResponseStreamEvent]:<br>        list_input = ItemHelpers.input_to_new_input_list(input)<br>        parallel_tool_calls = (<br>            True if model_settings.parallel_tool_calls and tools and len(tools) > 0 else NOT_GIVEN<br>        )<br>        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)<br>        converted_tools = Converter.convert_tools(tools, handoffs)<br>        response_format = Converter.get_response_format(output_schema)<br>        if _debug.DONT_LOG_MODEL_DATA:<br>            logger.debug("Calling LLM")<br>        else:<br>            logger.debug(<br>                f"Calling LLM {self.model} with input:\n"<br>                f"{json.dumps(list_input, indent=2)}\n"<br>                f"Tools:\n{json.dumps(converted_tools.tools, indent=2)}\n"<br>                f"Stream: {stream}\n"<br>                f"Tool choice: {tool_choice}\n"<br>                f"Response format: {response_format}\n"<br>            )<br>        return await self._client.responses.create(<br>            instructions=self._non_null_or_not_given(system_instructions),<br>            model=self.model,<br>            input=list_input,<br>            include=converted_tools.includes,<br>            tools=converted_tools.tools,<br>            temperature=self._non_null_or_not_given(model_settings.temperature),<br>            top_p=self._non_null_or_not_given(model_settings.top_p),<br>            truncation=self._non_null_or_not_given(model_settings.truncation),<br>            max_output_tokens=self._non_null_or_not_given(model_settings.max_tokens),<br>            tool_choice=tool_choice,<br>            parallel_tool_calls=parallel_tool_calls,<br>            stream=stream,<br>            extra_headers=_HEADERS,<br>            text=response_format,<br>        )<br>    def _get_client(self) -> AsyncOpenAI:<br>        if self._client is None:<br>            self._client = AsyncOpenAI()<br>        return self._client<br>``` |

#### stream\_response`async`

```md-code__content
stream_response(
    system_instructions: str | None,
    input: str | list[TResponseInputItem],
    model_settings: ModelSettings,
    tools: list[Tool],
    output_schema: AgentOutputSchema | None,
    handoffs: list[Handoff],
    tracing: ModelTracing,
) -> AsyncIterator[ResponseStreamEvent]

```

Yields a partial message as it is generated, as well as the usage information.

Source code in `src/agents/models/openai_responses.py`

|     |     |
| --- | --- |
| ```<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>``` | ```md-code__content<br>async def stream_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchema | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>) -> AsyncIterator[ResponseStreamEvent]:<br>    """<br>    Yields a partial message as it is generated, as well as the usage information.<br>    """<br>    with response_span(disabled=tracing.is_disabled()) as span_response:<br>        try:<br>            stream = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                stream=True,<br>            )<br>            final_response: Response | None = None<br>            async for chunk in stream:<br>                if isinstance(chunk, ResponseCompletedEvent):<br>                    final_response = chunk.response<br>                yield chunk<br>            if final_response and tracing.include_data():<br>                span_response.span_data.response = final_response<br>                span_response.span_data.input = input<br>        except Exception as e:<br>            span_response.set_error(<br>                SpanError(<br>                    message="Error streaming response",<br>                    data={<br>                        "error": str(e) if tracing.include_data() else e.__class__.__name__,<br>                    },<br>                )<br>            )<br>            logger.error(f"Error streaming response: {e}")<br>            raise<br>``` |

### Converter

Source code in `src/agents/models/openai_responses.py`

|     |     |
| --- | --- |
| ```<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>``` | ```md-code__content<br>class Converter:<br>    @classmethod<br>    def convert_tool_choice(<br>        cls, tool_choice: Literal["auto", "required", "none"] | str | None<br>    ) -> response_create_params.ToolChoice | NotGiven:<br>        if tool_choice is None:<br>            return NOT_GIVEN<br>        elif tool_choice == "required":<br>            return "required"<br>        elif tool_choice == "auto":<br>            return "auto"<br>        elif tool_choice == "none":<br>            return "none"<br>        elif tool_choice == "file_search":<br>            return {<br>                "type": "file_search",<br>            }<br>        elif tool_choice == "web_search_preview":<br>            return {<br>                "type": "web_search_preview",<br>            }<br>        elif tool_choice == "computer_use_preview":<br>            return {<br>                "type": "computer_use_preview",<br>            }<br>        else:<br>            return {<br>                "type": "function",<br>                "name": tool_choice,<br>            }<br>    @classmethod<br>    def get_response_format(<br>        cls, output_schema: AgentOutputSchema | None<br>    ) -> ResponseTextConfigParam | NotGiven:<br>        if output_schema is None or output_schema.is_plain_text():<br>            return NOT_GIVEN<br>        else:<br>            return {<br>                "format": {<br>                    "type": "json_schema",<br>                    "name": "final_output",<br>                    "schema": output_schema.json_schema(),<br>                    "strict": output_schema.strict_json_schema,<br>                }<br>            }<br>    @classmethod<br>    def convert_tools(<br>        cls,<br>        tools: list[Tool],<br>        handoffs: list[Handoff[Any]],<br>    ) -> ConvertedTools:<br>        converted_tools: list[ToolParam] = []<br>        includes: list[IncludeLiteral] = []<br>        computer_tools = [tool for tool in tools if isinstance(tool, ComputerTool)]<br>        if len(computer_tools) > 1:<br>            raise UserError(f"You can only provide one computer tool. Got {len(computer_tools)}")<br>        for tool in tools:<br>            converted_tool, include = cls._convert_tool(tool)<br>            converted_tools.append(converted_tool)<br>            if include:<br>                includes.append(include)<br>        for handoff in handoffs:<br>            converted_tools.append(cls._convert_handoff_tool(handoff))<br>        return ConvertedTools(tools=converted_tools, includes=includes)<br>    @classmethod<br>    def _convert_tool(cls, tool: Tool) -> tuple[ToolParam, IncludeLiteral | None]:<br>        """Returns converted tool and includes"""<br>        if isinstance(tool, FunctionTool):<br>            converted_tool: ToolParam = {<br>                "name": tool.name,<br>                "parameters": tool.params_json_schema,<br>                "strict": tool.strict_json_schema,<br>                "type": "function",<br>                "description": tool.description,<br>            }<br>            includes: IncludeLiteral | None = None<br>        elif isinstance(tool, WebSearchTool):<br>            ws: WebSearchToolParam = {<br>                "type": "web_search_preview",<br>                "user_location": tool.user_location,<br>                "search_context_size": tool.search_context_size,<br>            }<br>            converted_tool = ws<br>            includes = None<br>        elif isinstance(tool, FileSearchTool):<br>            converted_tool = {<br>                "type": "file_search",<br>                "vector_store_ids": tool.vector_store_ids,<br>            }<br>            if tool.max_num_results:<br>                converted_tool["max_num_results"] = tool.max_num_results<br>            if tool.ranking_options:<br>                converted_tool["ranking_options"] = tool.ranking_options<br>            if tool.filters:<br>                converted_tool["filters"] = tool.filters<br>            includes = "file_search_call.results" if tool.include_search_results else None<br>        elif isinstance(tool, ComputerTool):<br>            converted_tool = {<br>                "type": "computer_use_preview",<br>                "environment": tool.computer.environment,<br>                "display_width": tool.computer.dimensions[0],<br>                "display_height": tool.computer.dimensions[1],<br>            }<br>            includes = None<br>        else:<br>            raise UserError(f"Unknown tool type: {type(tool)}, tool")<br>        return converted_tool, includes<br>    @classmethod<br>    def _convert_handoff_tool(cls, handoff: Handoff) -> ToolParam:<br>        return {<br>            "name": handoff.tool_name,<br>            "parameters": handoff.input_json_schema,<br>            "strict": handoff.strict_json_schema,<br>            "type": "function",<br>            "description": handoff.tool_description,<br>        }<br>``` |