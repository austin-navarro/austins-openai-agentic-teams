[Skip to content](https://openai.github.io/openai-agents-python/ref/voice/models/openai_stt/#openai-stt)

# `OpenAI STT`

### OpenAISTTTranscriptionSession

Bases: `StreamedTranscriptionSession`

A transcription session for OpenAI's STT model.

Source code in `src/agents/voice/models/openai_stt.py`

|     |     |
| --- | --- |
| ```<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>``` | ```md-code__content<br>class OpenAISTTTranscriptionSession(StreamedTranscriptionSession):<br>    """A transcription session for OpenAI's STT model."""<br>    def __init__(<br>        self,<br>        input: StreamedAudioInput,<br>        client: AsyncOpenAI,<br>        model: str,<br>        settings: STTModelSettings,<br>        trace_include_sensitive_data: bool,<br>        trace_include_sensitive_audio_data: bool,<br>    ):<br>        self.connected: bool = False<br>        self._client = client<br>        self._model = model<br>        self._settings = settings<br>        self._turn_detection = settings.turn_detection or DEFAULT_TURN_DETECTION<br>        self._trace_include_sensitive_data = trace_include_sensitive_data<br>        self._trace_include_sensitive_audio_data = trace_include_sensitive_audio_data<br>        self._input_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32]] = input.queue<br>        self._output_queue: asyncio.Queue[str | ErrorSentinel | SessionCompleteSentinel] = (<br>            asyncio.Queue()<br>        )<br>        self._websocket: websockets.ClientConnection | None = None<br>        self._event_queue: asyncio.Queue[dict[str, Any] | WebsocketDoneSentinel] = asyncio.Queue()<br>        self._state_queue: asyncio.Queue[dict[str, Any]] = asyncio.Queue()<br>        self._turn_audio_buffer: list[npt.NDArray[np.int16 | np.float32]] = []<br>        self._tracing_span: Span[TranscriptionSpanData] | None = None<br>        # tasks<br>        self._listener_task: asyncio.Task[Any] | None = None<br>        self._process_events_task: asyncio.Task[Any] | None = None<br>        self._stream_audio_task: asyncio.Task[Any] | None = None<br>        self._connection_task: asyncio.Task[Any] | None = None<br>        self._stored_exception: Exception | None = None<br>    def _start_turn(self) -> None:<br>        self._tracing_span = transcription_span(<br>            model=self._model,<br>            model_config={<br>                "temperature": self._settings.temperature,<br>                "language": self._settings.language,<br>                "prompt": self._settings.prompt,<br>                "turn_detection": self._turn_detection,<br>            },<br>        )<br>        self._tracing_span.start()<br>    def _end_turn(self, _transcript: str) -> None:<br>        if len(_transcript) < 1:<br>            return<br>        if self._tracing_span:<br>            if self._trace_include_sensitive_audio_data:<br>                self._tracing_span.span_data.input = _audio_to_base64(self._turn_audio_buffer)<br>            self._tracing_span.span_data.input_format = "pcm"<br>            if self._trace_include_sensitive_data:<br>                self._tracing_span.span_data.output = _transcript<br>            self._tracing_span.finish()<br>            self._turn_audio_buffer = []<br>            self._tracing_span = None<br>    async def _event_listener(self) -> None:<br>        assert self._websocket is not None, "Websocket not initialized"<br>        async for message in self._websocket:<br>            try:<br>                event = json.loads(message)<br>                if event.get("type") == "error":<br>                    raise STTWebsocketConnectionError(f"Error event: {event.get('error')}")<br>                if event.get("type") in [<br>                    "session.updated",<br>                    "transcription_session.updated",<br>                    "session.created",<br>                    "transcription_session.created",<br>                ]:<br>                    await self._state_queue.put(event)<br>                await self._event_queue.put(event)<br>            except Exception as e:<br>                await self._output_queue.put(ErrorSentinel(e))<br>                raise STTWebsocketConnectionError("Error parsing events") from e<br>        await self._event_queue.put(WebsocketDoneSentinel())<br>    async def _configure_session(self) -> None:<br>        assert self._websocket is not None, "Websocket not initialized"<br>        await self._websocket.send(<br>            json.dumps(<br>                {<br>                    "type": "transcription_session.update",<br>                    "session": {<br>                        "input_audio_format": "pcm16",<br>                        "input_audio_transcription": {"model": self._model},<br>                        "turn_detection": self._turn_detection,<br>                    },<br>                }<br>            )<br>        )<br>    async def _setup_connection(self, ws: websockets.ClientConnection) -> None:<br>        self._websocket = ws<br>        self._listener_task = asyncio.create_task(self._event_listener())<br>        try:<br>            event = await _wait_for_event(<br>                self._state_queue,<br>                ["session.created", "transcription_session.created"],<br>                SESSION_CREATION_TIMEOUT,<br>            )<br>        except TimeoutError as e:<br>            wrapped_err = STTWebsocketConnectionError(<br>                "Timeout waiting for transcription_session.created event"<br>            )<br>            await self._output_queue.put(ErrorSentinel(wrapped_err))<br>            raise wrapped_err from e<br>        except Exception as e:<br>            await self._output_queue.put(ErrorSentinel(e))<br>            raise e<br>        await self._configure_session()<br>        try:<br>            event = await _wait_for_event(<br>                self._state_queue,<br>                ["session.updated", "transcription_session.updated"],<br>                SESSION_UPDATE_TIMEOUT,<br>            )<br>            if _debug.DONT_LOG_MODEL_DATA:<br>                logger.debug("Session updated")<br>            else:<br>                logger.debug(f"Session updated: {event}")<br>        except TimeoutError as e:<br>            wrapped_err = STTWebsocketConnectionError(<br>                "Timeout waiting for transcription_session.updated event"<br>            )<br>            await self._output_queue.put(ErrorSentinel(wrapped_err))<br>            raise wrapped_err from e<br>        except Exception as e:<br>            await self._output_queue.put(ErrorSentinel(e))<br>            raise<br>    async def _handle_events(self) -> None:<br>        while True:<br>            try:<br>                event = await asyncio.wait_for(<br>                    self._event_queue.get(), timeout=EVENT_INACTIVITY_TIMEOUT<br>                )<br>                if isinstance(event, WebsocketDoneSentinel):<br>                    # processed all events and websocket is done<br>                    break<br>                event_type = event.get("type", "unknown")<br>                if event_type == "conversation.item.input_audio_transcription.completed":<br>                    transcript = cast(str, event.get("transcript", ""))<br>                    if len(transcript) > 0:<br>                        self._end_turn(transcript)<br>                        self._start_turn()<br>                        await self._output_queue.put(transcript)<br>                await asyncio.sleep(0)  # yield control<br>            except asyncio.TimeoutError:<br>                # No new events for a while. Assume the session is done.<br>                break<br>            except Exception as e:<br>                await self._output_queue.put(ErrorSentinel(e))<br>                raise e<br>        await self._output_queue.put(SessionCompleteSentinel())<br>    async def _stream_audio(<br>        self, audio_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32]]<br>    ) -> None:<br>        assert self._websocket is not None, "Websocket not initialized"<br>        self._start_turn()<br>        while True:<br>            buffer = await audio_queue.get()<br>            if buffer is None:<br>                break<br>            self._turn_audio_buffer.append(buffer)<br>            try:<br>                await self._websocket.send(<br>                    json.dumps(<br>                        {<br>                            "type": "input_audio_buffer.append",<br>                            "audio": base64.b64encode(buffer.tobytes()).decode("utf-8"),<br>                        }<br>                    )<br>                )<br>            except websockets.ConnectionClosed:<br>                break<br>            except Exception as e:<br>                await self._output_queue.put(ErrorSentinel(e))<br>                raise e<br>            await asyncio.sleep(0)  # yield control<br>    async def _process_websocket_connection(self) -> None:<br>        try:<br>            async with websockets.connect(<br>                "wss://api.openai.com/v1/realtime?intent=transcription",<br>                additional_headers={<br>                    "Authorization": f"Bearer {self._client.api_key}",<br>                    "OpenAI-Beta": "realtime=v1",<br>                    "OpenAI-Log-Session": "1",<br>                },<br>            ) as ws:<br>                await self._setup_connection(ws)<br>                self._process_events_task = asyncio.create_task(self._handle_events())<br>                self._stream_audio_task = asyncio.create_task(self._stream_audio(self._input_queue))<br>                self.connected = True<br>                if self._listener_task:<br>                    await self._listener_task<br>                else:<br>                    logger.error("Listener task not initialized")<br>                    raise AgentsException("Listener task not initialized")<br>        except Exception as e:<br>            await self._output_queue.put(ErrorSentinel(e))<br>            raise e<br>    def _check_errors(self) -> None:<br>        if self._connection_task and self._connection_task.done():<br>            exc = self._connection_task.exception()<br>            if exc and isinstance(exc, Exception):<br>                self._stored_exception = exc<br>        if self._process_events_task and self._process_events_task.done():<br>            exc = self._process_events_task.exception()<br>            if exc and isinstance(exc, Exception):<br>                self._stored_exception = exc<br>        if self._stream_audio_task and self._stream_audio_task.done():<br>            exc = self._stream_audio_task.exception()<br>            if exc and isinstance(exc, Exception):<br>                self._stored_exception = exc<br>        if self._listener_task and self._listener_task.done():<br>            exc = self._listener_task.exception()<br>            if exc and isinstance(exc, Exception):<br>                self._stored_exception = exc<br>    def _cleanup_tasks(self) -> None:<br>        if self._listener_task and not self._listener_task.done():<br>            self._listener_task.cancel()<br>        if self._process_events_task and not self._process_events_task.done():<br>            self._process_events_task.cancel()<br>        if self._stream_audio_task and not self._stream_audio_task.done():<br>            self._stream_audio_task.cancel()<br>        if self._connection_task and not self._connection_task.done():<br>            self._connection_task.cancel()<br>    async def transcribe_turns(self) -> AsyncIterator[str]:<br>        self._connection_task = asyncio.create_task(self._process_websocket_connection())<br>        while True:<br>            try:<br>                turn = await self._output_queue.get()<br>            except asyncio.CancelledError:<br>                break<br>            if (<br>                turn is None<br>                or isinstance(turn, ErrorSentinel)<br>                or isinstance(turn, SessionCompleteSentinel)<br>            ):<br>                self._output_queue.task_done()<br>                break<br>            yield turn<br>            self._output_queue.task_done()<br>        if self._tracing_span:<br>            self._end_turn("")<br>        if self._websocket:<br>            await self._websocket.close()<br>        self._check_errors()<br>        if self._stored_exception:<br>            raise self._stored_exception<br>    async def close(self) -> None:<br>        if self._websocket:<br>            await self._websocket.close()<br>        self._cleanup_tasks()<br>``` |

### OpenAISTTModel

Bases: `STTModel`

A speech-to-text model for OpenAI.

Source code in `src/agents/voice/models/openai_stt.py`

|     |     |
| --- | --- |
| ```<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>``` | ```md-code__content<br>class OpenAISTTModel(STTModel):<br>    """A speech-to-text model for OpenAI."""<br>    def __init__(<br>        self,<br>        model: str,<br>        openai_client: AsyncOpenAI,<br>    ):<br>        """Create a new OpenAI speech-to-text model.<br>        Args:<br>            model: The name of the model to use.<br>            openai_client: The OpenAI client to use.<br>        """<br>        self.model = model<br>        self._client = openai_client<br>    @property<br>    def model_name(self) -> str:<br>        return self.model<br>    def _non_null_or_not_given(self, value: Any) -> Any:<br>        return value if value is not None else None  # NOT_GIVEN<br>    async def transcribe(<br>        self,<br>        input: AudioInput,<br>        settings: STTModelSettings,<br>        trace_include_sensitive_data: bool,<br>        trace_include_sensitive_audio_data: bool,<br>    ) -> str:<br>        """Transcribe an audio input.<br>        Args:<br>            input: The audio input to transcribe.<br>            settings: The settings to use for the transcription.<br>        Returns:<br>            The transcribed text.<br>        """<br>        with transcription_span(<br>            model=self.model,<br>            input=input.to_base64() if trace_include_sensitive_audio_data else "",<br>            input_format="pcm",<br>            model_config={<br>                "temperature": self._non_null_or_not_given(settings.temperature),<br>                "language": self._non_null_or_not_given(settings.language),<br>                "prompt": self._non_null_or_not_given(settings.prompt),<br>            },<br>        ) as span:<br>            try:<br>                response = await self._client.audio.transcriptions.create(<br>                    model=self.model,<br>                    file=input.to_audio_file(),<br>                    prompt=self._non_null_or_not_given(settings.prompt),<br>                    language=self._non_null_or_not_given(settings.language),<br>                    temperature=self._non_null_or_not_given(settings.temperature),<br>                )<br>                if trace_include_sensitive_data:<br>                    span.span_data.output = response.text<br>                return response.text<br>            except Exception as e:<br>                span.span_data.output = ""<br>                span.set_error(SpanError(message=str(e), data={}))<br>                raise e<br>    async def create_session(<br>        self,<br>        input: StreamedAudioInput,<br>        settings: STTModelSettings,<br>        trace_include_sensitive_data: bool,<br>        trace_include_sensitive_audio_data: bool,<br>    ) -> StreamedTranscriptionSession:<br>        """Create a new transcription session.<br>        Args:<br>            input: The audio input to transcribe.<br>            settings: The settings to use for the transcription.<br>            trace_include_sensitive_data: Whether to include sensitive data in traces.<br>            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.<br>        Returns:<br>            A new transcription session.<br>        """<br>        return OpenAISTTTranscriptionSession(<br>            input,<br>            self._client,<br>            self.model,<br>            settings,<br>            trace_include_sensitive_data,<br>            trace_include_sensitive_audio_data,<br>        )<br>``` |

#### \_\_init\_\_

```md-code__content
__init__(model: str, openai_client: AsyncOpenAI)

```

Create a new OpenAI speech-to-text model.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `model` | `str` | The name of the model to use. | _required_ |
| `openai_client` | `AsyncOpenAI` | The OpenAI client to use. | _required_ |

Source code in `src/agents/voice/models/openai_stt.py`

|     |     |
| --- | --- |
| ```<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>``` | ```md-code__content<br>def __init__(<br>    self,<br>    model: str,<br>    openai_client: AsyncOpenAI,<br>):<br>    """Create a new OpenAI speech-to-text model.<br>    Args:<br>        model: The name of the model to use.<br>        openai_client: The OpenAI client to use.<br>    """<br>    self.model = model<br>    self._client = openai_client<br>``` |

#### transcribe`async`

```md-code__content
transcribe(
    input: AudioInput,
    settings: STTModelSettings,
    trace_include_sensitive_data: bool,
    trace_include_sensitive_audio_data: bool,
) -> str

```

Transcribe an audio input.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `input` | `AudioInput` | The audio input to transcribe. | _required_ |
| `settings` | `STTModelSettings` | The settings to use for the transcription. | _required_ |

Returns:

| Type | Description |
| --- | --- |
| `str` | The transcribed text. |

Source code in `src/agents/voice/models/openai_stt.py`

|     |     |
| --- | --- |
| ```<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>``` | ```md-code__content<br>async def transcribe(<br>    self,<br>    input: AudioInput,<br>    settings: STTModelSettings,<br>    trace_include_sensitive_data: bool,<br>    trace_include_sensitive_audio_data: bool,<br>) -> str:<br>    """Transcribe an audio input.<br>    Args:<br>        input: The audio input to transcribe.<br>        settings: The settings to use for the transcription.<br>    Returns:<br>        The transcribed text.<br>    """<br>    with transcription_span(<br>        model=self.model,<br>        input=input.to_base64() if trace_include_sensitive_audio_data else "",<br>        input_format="pcm",<br>        model_config={<br>            "temperature": self._non_null_or_not_given(settings.temperature),<br>            "language": self._non_null_or_not_given(settings.language),<br>            "prompt": self._non_null_or_not_given(settings.prompt),<br>        },<br>    ) as span:<br>        try:<br>            response = await self._client.audio.transcriptions.create(<br>                model=self.model,<br>                file=input.to_audio_file(),<br>                prompt=self._non_null_or_not_given(settings.prompt),<br>                language=self._non_null_or_not_given(settings.language),<br>                temperature=self._non_null_or_not_given(settings.temperature),<br>            )<br>            if trace_include_sensitive_data:<br>                span.span_data.output = response.text<br>            return response.text<br>        except Exception as e:<br>            span.span_data.output = ""<br>            span.set_error(SpanError(message=str(e), data={}))<br>            raise e<br>``` |

#### create\_session`async`

```md-code__content
create_session(
    input: StreamedAudioInput,
    settings: STTModelSettings,
    trace_include_sensitive_data: bool,
    trace_include_sensitive_audio_data: bool,
) -> StreamedTranscriptionSession

```

Create a new transcription session.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `input` | `StreamedAudioInput` | The audio input to transcribe. | _required_ |
| `settings` | `STTModelSettings` | The settings to use for the transcription. | _required_ |
| `trace_include_sensitive_data` | `bool` | Whether to include sensitive data in traces. | _required_ |
| `trace_include_sensitive_audio_data` | `bool` | Whether to include sensitive audio data in traces. | _required_ |

Returns:

| Type | Description |
| --- | --- |
| `StreamedTranscriptionSession` | A new transcription session. |

Source code in `src/agents/voice/models/openai_stt.py`

|     |     |
| --- | --- |
| ```<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>``` | ```md-code__content<br>async def create_session(<br>    self,<br>    input: StreamedAudioInput,<br>    settings: STTModelSettings,<br>    trace_include_sensitive_data: bool,<br>    trace_include_sensitive_audio_data: bool,<br>) -> StreamedTranscriptionSession:<br>    """Create a new transcription session.<br>    Args:<br>        input: The audio input to transcribe.<br>        settings: The settings to use for the transcription.<br>        trace_include_sensitive_data: Whether to include sensitive data in traces.<br>        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.<br>    Returns:<br>        A new transcription session.<br>    """<br>    return OpenAISTTTranscriptionSession(<br>        input,<br>        self._client,<br>        self.model,<br>        settings,<br>        trace_include_sensitive_data,<br>        trace_include_sensitive_audio_data,<br>    )<br>``` |