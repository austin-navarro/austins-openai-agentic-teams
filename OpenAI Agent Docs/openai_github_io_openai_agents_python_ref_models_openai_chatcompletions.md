[Skip to content](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#openai-chat-completions-model)

# `OpenAI Chat Completions model`

### OpenAIChatCompletionsModel

Bases: `Model`

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>``` | ```md-code__content<br>class OpenAIChatCompletionsModel(Model):<br>    def __init__(<br>        self,<br>        model: str | ChatModel,<br>        openai_client: AsyncOpenAI,<br>    ) -> None:<br>        self.model = model<br>        self._client = openai_client<br>    def _non_null_or_not_given(self, value: Any) -> Any:<br>        return value if value is not None else NOT_GIVEN<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> ModelResponse:<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=dataclasses.asdict(model_settings)<br>            | {"base_url": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=False,<br>            )<br>            if _debug.DONT_LOG_MODEL_DATA:<br>                logger.debug("Received model response")<br>            else:<br>                logger.debug(<br>                    f"LLM resp:\n{json.dumps(response.choices[0].message.model_dump(), indent=2)}\n"<br>                )<br>            usage = (<br>                Usage(<br>                    requests=1,<br>                    input_tokens=response.usage.prompt_tokens,<br>                    output_tokens=response.usage.completion_tokens,<br>                    total_tokens=response.usage.total_tokens,<br>                )<br>                if response.usage<br>                else Usage()<br>            )<br>            if tracing.include_data():<br>                span_generation.span_data.output = [response.choices[0].message.model_dump()]<br>            span_generation.span_data.usage = {<br>                "input_tokens": usage.input_tokens,<br>                "output_tokens": usage.output_tokens,<br>            }<br>            items = _Converter.message_to_output_items(response.choices[0].message)<br>            return ModelResponse(<br>                output=items,<br>                usage=usage,<br>                referenceable_id=None,<br>            )<br>    async def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> AsyncIterator[TResponseStreamEvent]:<br>        """<br>        Yields a partial message as it is generated, as well as the usage information.<br>        """<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=dataclasses.asdict(model_settings)<br>            | {"base_url": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response, stream = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=True,<br>            )<br>            usage: CompletionUsage | None = None<br>            state = _StreamingState()<br>            async for chunk in stream:<br>                if not state.started:<br>                    state.started = True<br>                    yield ResponseCreatedEvent(<br>                        response=response,<br>                        type="response.created",<br>                    )<br>                # The usage is only available in the last chunk<br>                usage = chunk.usage<br>                if not chunk.choices or not chunk.choices[0].delta:<br>                    continue<br>                delta = chunk.choices[0].delta<br>                # Handle text<br>                if delta.content:<br>                    if not state.text_content_index_and_output:<br>                        # Initialize a content tracker for streaming text<br>                        state.text_content_index_and_output = (<br>                            0 if not state.refusal_content_index_and_output else 1,<br>                            ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                        )<br>                        # Start a new assistant message stream<br>                        assistant_item = ResponseOutputMessage(<br>                            id=FAKE_RESPONSES_ID,<br>                            content=[],<br>                            role="assistant",<br>                            type="message",<br>                            status="in_progress",<br>                        )<br>                        # Notify consumers of the start of a new output message + first content part<br>                        yield ResponseOutputItemAddedEvent(<br>                            item=assistant_item,<br>                            output_index=0,<br>                            type="response.output_item.added",<br>                        )<br>                        yield ResponseContentPartAddedEvent(<br>                            content_index=state.text_content_index_and_output[0],<br>                            item_id=FAKE_RESPONSES_ID,<br>                            output_index=0,<br>                            part=ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                            type="response.content_part.added",<br>                        )<br>                    # Emit the delta for this segment of content<br>                    yield ResponseTextDeltaEvent(<br>                        content_index=state.text_content_index_and_output[0],<br>                        delta=delta.content,<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        type="response.output_text.delta",<br>                    )<br>                    # Accumulate the text into the response part<br>                    state.text_content_index_and_output[1].text += delta.content<br>                # Handle refusals (model declines to answer)<br>                if delta.refusal:<br>                    if not state.refusal_content_index_and_output:<br>                        # Initialize a content tracker for streaming refusal text<br>                        state.refusal_content_index_and_output = (<br>                            0 if not state.text_content_index_and_output else 1,<br>                            ResponseOutputRefusal(refusal="", type="refusal"),<br>                        )<br>                        # Start a new assistant message if one doesn't exist yet (in-progress)<br>                        assistant_item = ResponseOutputMessage(<br>                            id=FAKE_RESPONSES_ID,<br>                            content=[],<br>                            role="assistant",<br>                            type="message",<br>                            status="in_progress",<br>                        )<br>                        # Notify downstream that assistant message + first content part are starting<br>                        yield ResponseOutputItemAddedEvent(<br>                            item=assistant_item,<br>                            output_index=0,<br>                            type="response.output_item.added",<br>                        )<br>                        yield ResponseContentPartAddedEvent(<br>                            content_index=state.refusal_content_index_and_output[0],<br>                            item_id=FAKE_RESPONSES_ID,<br>                            output_index=0,<br>                            part=ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                            type="response.content_part.added",<br>                        )<br>                    # Emit the delta for this segment of refusal<br>                    yield ResponseRefusalDeltaEvent(<br>                        content_index=state.refusal_content_index_and_output[0],<br>                        delta=delta.refusal,<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        type="response.refusal.delta",<br>                    )<br>                    # Accumulate the refusal string in the output part<br>                    state.refusal_content_index_and_output[1].refusal += delta.refusal<br>                # Handle tool calls<br>                # Because we don't know the name of the function until the end of the stream, we'll<br>                # save everything and yield events at the end<br>                if delta.tool_calls:<br>                    for tc_delta in delta.tool_calls:<br>                        if tc_delta.index not in state.function_calls:<br>                            state.function_calls[tc_delta.index] = ResponseFunctionToolCall(<br>                                id=FAKE_RESPONSES_ID,<br>                                arguments="",<br>                                name="",<br>                                type="function_call",<br>                                call_id="",<br>                            )<br>                        tc_function = tc_delta.function<br>                        state.function_calls[tc_delta.index].arguments += (<br>                            tc_function.arguments if tc_function else ""<br>                        ) or ""<br>                        state.function_calls[tc_delta.index].name += (<br>                            tc_function.name if tc_function else ""<br>                        ) or ""<br>                        state.function_calls[tc_delta.index].call_id += tc_delta.id or ""<br>            function_call_starting_index = 0<br>            if state.text_content_index_and_output:<br>                function_call_starting_index += 1<br>                # Send end event for this content part<br>                yield ResponseContentPartDoneEvent(<br>                    content_index=state.text_content_index_and_output[0],<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    part=state.text_content_index_and_output[1],<br>                    type="response.content_part.done",<br>                )<br>            if state.refusal_content_index_and_output:<br>                function_call_starting_index += 1<br>                # Send end event for this content part<br>                yield ResponseContentPartDoneEvent(<br>                    content_index=state.refusal_content_index_and_output[0],<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    part=state.refusal_content_index_and_output[1],<br>                    type="response.content_part.done",<br>                )<br>            # Actually send events for the function calls<br>            for function_call in state.function_calls.values():<br>                # First, a ResponseOutputItemAdded for the function call<br>                yield ResponseOutputItemAddedEvent(<br>                    item=ResponseFunctionToolCall(<br>                        id=FAKE_RESPONSES_ID,<br>                        call_id=function_call.call_id,<br>                        arguments=function_call.arguments,<br>                        name=function_call.name,<br>                        type="function_call",<br>                    ),<br>                    output_index=function_call_starting_index,<br>                    type="response.output_item.added",<br>                )<br>                # Then, yield the args<br>                yield ResponseFunctionCallArgumentsDeltaEvent(<br>                    delta=function_call.arguments,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=function_call_starting_index,<br>                    type="response.function_call_arguments.delta",<br>                )<br>                # Finally, the ResponseOutputItemDone<br>                yield ResponseOutputItemDoneEvent(<br>                    item=ResponseFunctionToolCall(<br>                        id=FAKE_RESPONSES_ID,<br>                        call_id=function_call.call_id,<br>                        arguments=function_call.arguments,<br>                        name=function_call.name,<br>                        type="function_call",<br>                    ),<br>                    output_index=function_call_starting_index,<br>                    type="response.output_item.done",<br>                )<br>            # Finally, send the Response completed event<br>            outputs: list[ResponseOutputItem] = []<br>            if state.text_content_index_and_output or state.refusal_content_index_and_output:<br>                assistant_msg = ResponseOutputMessage(<br>                    id=FAKE_RESPONSES_ID,<br>                    content=[],<br>                    role="assistant",<br>                    type="message",<br>                    status="completed",<br>                )<br>                if state.text_content_index_and_output:<br>                    assistant_msg.content.append(state.text_content_index_and_output[1])<br>                if state.refusal_content_index_and_output:<br>                    assistant_msg.content.append(state.refusal_content_index_and_output[1])<br>                outputs.append(assistant_msg)<br>                # send a ResponseOutputItemDone for the assistant message<br>                yield ResponseOutputItemDoneEvent(<br>                    item=assistant_msg,<br>                    output_index=0,<br>                    type="response.output_item.done",<br>                )<br>            for function_call in state.function_calls.values():<br>                outputs.append(function_call)<br>            final_response = response.model_copy()<br>            final_response.output = outputs<br>            final_response.usage = (<br>                ResponseUsage(<br>                    input_tokens=usage.prompt_tokens,<br>                    output_tokens=usage.completion_tokens,<br>                    total_tokens=usage.total_tokens,<br>                    output_tokens_details=OutputTokensDetails(<br>                        reasoning_tokens=usage.completion_tokens_details.reasoning_tokens<br>                        if usage.completion_tokens_details<br>                        and usage.completion_tokens_details.reasoning_tokens<br>                        else 0<br>                    ),<br>                    input_tokens_details=InputTokensDetails(<br>                        cached_tokens=usage.prompt_tokens_details.cached_tokens<br>                        if usage.prompt_tokens_details and usage.prompt_tokens_details.cached_tokens<br>                        else 0<br>                    ),<br>                )<br>                if usage<br>                else None<br>            )<br>            yield ResponseCompletedEvent(<br>                response=final_response,<br>                type="response.completed",<br>            )<br>            if tracing.include_data():<br>                span_generation.span_data.output = [final_response.model_dump()]<br>            if usage:<br>                span_generation.span_data.usage = {<br>                    "input_tokens": usage.prompt_tokens,<br>                    "output_tokens": usage.completion_tokens,<br>                }<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[True],<br>    ) -> tuple[Response, AsyncStream[ChatCompletionChunk]]: ...<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[False],<br>    ) -> ChatCompletion: ...<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: bool = False,<br>    ) -> ChatCompletion | tuple[Response, AsyncStream[ChatCompletionChunk]]:<br>        converted_messages = _Converter.items_to_messages(input)<br>        if system_instructions:<br>            converted_messages.insert(<br>                0,<br>                {<br>                    "content": system_instructions,<br>                    "role": "system",<br>                },<br>            )<br>        if tracing.include_data():<br>            span.span_data.input = converted_messages<br>        parallel_tool_calls = (<br>            True if model_settings.parallel_tool_calls and tools and len(tools) > 0 else NOT_GIVEN<br>        )<br>        tool_choice = _Converter.convert_tool_choice(model_settings.tool_choice)<br>        response_format = _Converter.convert_response_format(output_schema)<br>        converted_tools = [ToolConverter.to_openai(tool) for tool in tools] if tools else []<br>        for handoff in handoffs:<br>            converted_tools.append(ToolConverter.convert_handoff_tool(handoff))<br>        if _debug.DONT_LOG_MODEL_DATA:<br>            logger.debug("Calling LLM")<br>        else:<br>            logger.debug(<br>                f"{json.dumps(converted_messages, indent=2)}\n"<br>                f"Tools:\n{json.dumps(converted_tools, indent=2)}\n"<br>                f"Stream: {stream}\n"<br>                f"Tool choice: {tool_choice}\n"<br>                f"Response format: {response_format}\n"<br>            )<br>        ret = await self._get_client().chat.completions.create(<br>            model=self.model,<br>            messages=converted_messages,<br>            tools=converted_tools or NOT_GIVEN,<br>            temperature=self._non_null_or_not_given(model_settings.temperature),<br>            top_p=self._non_null_or_not_given(model_settings.top_p),<br>            frequency_penalty=self._non_null_or_not_given(model_settings.frequency_penalty),<br>            presence_penalty=self._non_null_or_not_given(model_settings.presence_penalty),<br>            max_tokens=self._non_null_or_not_given(model_settings.max_tokens),<br>            tool_choice=tool_choice,<br>            response_format=response_format,<br>            parallel_tool_calls=parallel_tool_calls,<br>            stream=stream,<br>            stream_options={"include_usage": True} if stream else NOT_GIVEN,<br>            extra_headers=_HEADERS,<br>        )<br>        if isinstance(ret, ChatCompletion):<br>            return ret<br>        response = Response(<br>            id=FAKE_RESPONSES_ID,<br>            created_at=time.time(),<br>            model=self.model,<br>            object="response",<br>            output=[],<br>            tool_choice=cast(Literal["auto", "required", "none"], tool_choice)<br>            if tool_choice != NOT_GIVEN<br>            else "auto",<br>            top_p=model_settings.top_p,<br>            temperature=model_settings.temperature,<br>            tools=[],<br>            parallel_tool_calls=parallel_tool_calls or False,<br>        )<br>        return response, ret<br>    def _get_client(self) -> AsyncOpenAI:<br>        if self._client is None:<br>            self._client = AsyncOpenAI()<br>        return self._client<br>``` |

#### stream\_response`async`

```md-code__content
stream_response(
    system_instructions: str | None,
    input: str | list[TResponseInputItem],
    model_settings: ModelSettings,
    tools: list[Tool],
    output_schema: AgentOutputSchema | None,
    handoffs: list[Handoff],
    tracing: ModelTracing,
) -> AsyncIterator[TResponseStreamEvent]

```

Yields a partial message as it is generated, as well as the usage information.

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>``` | ```md-code__content<br>async def stream_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchema | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>) -> AsyncIterator[TResponseStreamEvent]:<br>    """<br>    Yields a partial message as it is generated, as well as the usage information.<br>    """<br>    with generation_span(<br>        model=str(self.model),<br>        model_config=dataclasses.asdict(model_settings)<br>        | {"base_url": str(self._client.base_url)},<br>        disabled=tracing.is_disabled(),<br>    ) as span_generation:<br>        response, stream = await self._fetch_response(<br>            system_instructions,<br>            input,<br>            model_settings,<br>            tools,<br>            output_schema,<br>            handoffs,<br>            span_generation,<br>            tracing,<br>            stream=True,<br>        )<br>        usage: CompletionUsage | None = None<br>        state = _StreamingState()<br>        async for chunk in stream:<br>            if not state.started:<br>                state.started = True<br>                yield ResponseCreatedEvent(<br>                    response=response,<br>                    type="response.created",<br>                )<br>            # The usage is only available in the last chunk<br>            usage = chunk.usage<br>            if not chunk.choices or not chunk.choices[0].delta:<br>                continue<br>            delta = chunk.choices[0].delta<br>            # Handle text<br>            if delta.content:<br>                if not state.text_content_index_and_output:<br>                    # Initialize a content tracker for streaming text<br>                    state.text_content_index_and_output = (<br>                        0 if not state.refusal_content_index_and_output else 1,<br>                        ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                    )<br>                    # Start a new assistant message stream<br>                    assistant_item = ResponseOutputMessage(<br>                        id=FAKE_RESPONSES_ID,<br>                        content=[],<br>                        role="assistant",<br>                        type="message",<br>                        status="in_progress",<br>                    )<br>                    # Notify consumers of the start of a new output message + first content part<br>                    yield ResponseOutputItemAddedEvent(<br>                        item=assistant_item,<br>                        output_index=0,<br>                        type="response.output_item.added",<br>                    )<br>                    yield ResponseContentPartAddedEvent(<br>                        content_index=state.text_content_index_and_output[0],<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        part=ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                        type="response.content_part.added",<br>                    )<br>                # Emit the delta for this segment of content<br>                yield ResponseTextDeltaEvent(<br>                    content_index=state.text_content_index_and_output[0],<br>                    delta=delta.content,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    type="response.output_text.delta",<br>                )<br>                # Accumulate the text into the response part<br>                state.text_content_index_and_output[1].text += delta.content<br>            # Handle refusals (model declines to answer)<br>            if delta.refusal:<br>                if not state.refusal_content_index_and_output:<br>                    # Initialize a content tracker for streaming refusal text<br>                    state.refusal_content_index_and_output = (<br>                        0 if not state.text_content_index_and_output else 1,<br>                        ResponseOutputRefusal(refusal="", type="refusal"),<br>                    )<br>                    # Start a new assistant message if one doesn't exist yet (in-progress)<br>                    assistant_item = ResponseOutputMessage(<br>                        id=FAKE_RESPONSES_ID,<br>                        content=[],<br>                        role="assistant",<br>                        type="message",<br>                        status="in_progress",<br>                    )<br>                    # Notify downstream that assistant message + first content part are starting<br>                    yield ResponseOutputItemAddedEvent(<br>                        item=assistant_item,<br>                        output_index=0,<br>                        type="response.output_item.added",<br>                    )<br>                    yield ResponseContentPartAddedEvent(<br>                        content_index=state.refusal_content_index_and_output[0],<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        part=ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                        type="response.content_part.added",<br>                    )<br>                # Emit the delta for this segment of refusal<br>                yield ResponseRefusalDeltaEvent(<br>                    content_index=state.refusal_content_index_and_output[0],<br>                    delta=delta.refusal,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    type="response.refusal.delta",<br>                )<br>                # Accumulate the refusal string in the output part<br>                state.refusal_content_index_and_output[1].refusal += delta.refusal<br>            # Handle tool calls<br>            # Because we don't know the name of the function until the end of the stream, we'll<br>            # save everything and yield events at the end<br>            if delta.tool_calls:<br>                for tc_delta in delta.tool_calls:<br>                    if tc_delta.index not in state.function_calls:<br>                        state.function_calls[tc_delta.index] = ResponseFunctionToolCall(<br>                            id=FAKE_RESPONSES_ID,<br>                            arguments="",<br>                            name="",<br>                            type="function_call",<br>                            call_id="",<br>                        )<br>                    tc_function = tc_delta.function<br>                    state.function_calls[tc_delta.index].arguments += (<br>                        tc_function.arguments if tc_function else ""<br>                    ) or ""<br>                    state.function_calls[tc_delta.index].name += (<br>                        tc_function.name if tc_function else ""<br>                    ) or ""<br>                    state.function_calls[tc_delta.index].call_id += tc_delta.id or ""<br>        function_call_starting_index = 0<br>        if state.text_content_index_and_output:<br>            function_call_starting_index += 1<br>            # Send end event for this content part<br>            yield ResponseContentPartDoneEvent(<br>                content_index=state.text_content_index_and_output[0],<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=0,<br>                part=state.text_content_index_and_output[1],<br>                type="response.content_part.done",<br>            )<br>        if state.refusal_content_index_and_output:<br>            function_call_starting_index += 1<br>            # Send end event for this content part<br>            yield ResponseContentPartDoneEvent(<br>                content_index=state.refusal_content_index_and_output[0],<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=0,<br>                part=state.refusal_content_index_and_output[1],<br>                type="response.content_part.done",<br>            )<br>        # Actually send events for the function calls<br>        for function_call in state.function_calls.values():<br>            # First, a ResponseOutputItemAdded for the function call<br>            yield ResponseOutputItemAddedEvent(<br>                item=ResponseFunctionToolCall(<br>                    id=FAKE_RESPONSES_ID,<br>                    call_id=function_call.call_id,<br>                    arguments=function_call.arguments,<br>                    name=function_call.name,<br>                    type="function_call",<br>                ),<br>                output_index=function_call_starting_index,<br>                type="response.output_item.added",<br>            )<br>            # Then, yield the args<br>            yield ResponseFunctionCallArgumentsDeltaEvent(<br>                delta=function_call.arguments,<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=function_call_starting_index,<br>                type="response.function_call_arguments.delta",<br>            )<br>            # Finally, the ResponseOutputItemDone<br>            yield ResponseOutputItemDoneEvent(<br>                item=ResponseFunctionToolCall(<br>                    id=FAKE_RESPONSES_ID,<br>                    call_id=function_call.call_id,<br>                    arguments=function_call.arguments,<br>                    name=function_call.name,<br>                    type="function_call",<br>                ),<br>                output_index=function_call_starting_index,<br>                type="response.output_item.done",<br>            )<br>        # Finally, send the Response completed event<br>        outputs: list[ResponseOutputItem] = []<br>        if state.text_content_index_and_output or state.refusal_content_index_and_output:<br>            assistant_msg = ResponseOutputMessage(<br>                id=FAKE_RESPONSES_ID,<br>                content=[],<br>                role="assistant",<br>                type="message",<br>                status="completed",<br>            )<br>            if state.text_content_index_and_output:<br>                assistant_msg.content.append(state.text_content_index_and_output[1])<br>            if state.refusal_content_index_and_output:<br>                assistant_msg.content.append(state.refusal_content_index_and_output[1])<br>            outputs.append(assistant_msg)<br>            # send a ResponseOutputItemDone for the assistant message<br>            yield ResponseOutputItemDoneEvent(<br>                item=assistant_msg,<br>                output_index=0,<br>                type="response.output_item.done",<br>            )<br>        for function_call in state.function_calls.values():<br>            outputs.append(function_call)<br>        final_response = response.model_copy()<br>        final_response.output = outputs<br>        final_response.usage = (<br>            ResponseUsage(<br>                input_tokens=usage.prompt_tokens,<br>                output_tokens=usage.completion_tokens,<br>                total_tokens=usage.total_tokens,<br>                output_tokens_details=OutputTokensDetails(<br>                    reasoning_tokens=usage.completion_tokens_details.reasoning_tokens<br>                    if usage.completion_tokens_details<br>                    and usage.completion_tokens_details.reasoning_tokens<br>                    else 0<br>                ),<br>                input_tokens_details=InputTokensDetails(<br>                    cached_tokens=usage.prompt_tokens_details.cached_tokens<br>                    if usage.prompt_tokens_details and usage.prompt_tokens_details.cached_tokens<br>                    else 0<br>                ),<br>            )<br>            if usage<br>            else None<br>        )<br>        yield ResponseCompletedEvent(<br>            response=final_response,<br>            type="response.completed",<br>        )<br>        if tracing.include_data():<br>            span_generation.span_data.output = [final_response.model_dump()]<br>        if usage:<br>            span_generation.span_data.usage = {<br>                "input_tokens": usage.prompt_tokens,<br>                "output_tokens": usage.completion_tokens,<br>            }<br>``` |