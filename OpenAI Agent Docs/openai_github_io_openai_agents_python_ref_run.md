[Skip to content](https://openai.github.io/openai-agents-python/ref/run/#runner)

# `Runner`

### Runner

Source code in `src/agents/run.py`

|     |     |
| --- | --- |
| ```<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>831<br>832<br>833<br>834<br>835<br>836<br>837<br>838<br>839<br>840<br>841<br>842<br>843<br>844<br>845<br>846<br>847<br>848<br>849<br>850<br>851<br>852<br>853<br>854<br>855<br>856<br>857<br>858<br>859<br>860<br>861<br>862<br>863<br>864<br>865<br>866<br>867<br>868<br>869<br>870<br>871<br>872<br>873<br>874<br>875<br>876<br>877<br>878<br>879<br>880<br>881<br>882<br>883<br>884<br>885<br>886<br>887<br>888<br>889<br>890<br>891<br>892<br>893<br>894<br>895<br>896<br>897<br>898<br>899<br>900<br>901<br>902<br>903<br>904<br>``` | ```md-code__content<br>class Runner:<br>    @classmethod<br>    async def run(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        *,<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>    ) -> RunResult:<br>        """Run a workflow starting at the given agent. The agent will run in a loop until a final<br>        output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>        Returns:<br>            A run result containing all the inputs, guardrail results and the output of the last<br>            agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>        """<br>        if hooks is None:<br>            hooks = RunHooks[Any]()<br>        if run_config is None:<br>            run_config = RunConfig()<br>        with TraceCtxManager(<br>            workflow_name=run_config.workflow_name,<br>            trace_id=run_config.trace_id,<br>            group_id=run_config.group_id,<br>            metadata=run_config.trace_metadata,<br>            disabled=run_config.tracing_disabled,<br>        ):<br>            current_turn = 0<br>            original_input: str | list[TResponseInputItem] = copy.deepcopy(input)<br>            generated_items: list[RunItem] = []<br>            model_responses: list[ModelResponse] = []<br>            context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(<br>                context=context,  # type: ignore<br>            )<br>            input_guardrail_results: list[InputGuardrailResult] = []<br>            current_span: Span[AgentSpanData] | None = None<br>            current_agent = starting_agent<br>            should_run_agent_start_hooks = True<br>            try:<br>                while True:<br>                    # Start an agent span if we don't have one. This span is ended if the current<br>                    # agent changes, or if the agent loop ends.<br>                    if current_span is None:<br>                        handoff_names = [h.agent_name for h in cls._get_handoffs(current_agent)]<br>                        tool_names = [t.name for t in current_agent.tools]<br>                        if output_schema := cls._get_output_schema(current_agent):<br>                            output_type_name = output_schema.output_type_name()<br>                        else:<br>                            output_type_name = "str"<br>                        current_span = agent_span(<br>                            name=current_agent.name,<br>                            handoffs=handoff_names,<br>                            tools=tool_names,<br>                            output_type=output_type_name,<br>                        )<br>                        current_span.start(mark_as_current=True)<br>                    current_turn += 1<br>                    if current_turn > max_turns:<br>                        _error_tracing.attach_error_to_span(<br>                            current_span,<br>                            SpanError(<br>                                message="Max turns exceeded",<br>                                data={"max_turns": max_turns},<br>                            ),<br>                        )<br>                        raise MaxTurnsExceeded(f"Max turns ({max_turns}) exceeded")<br>                    logger.debug(<br>                        f"Running agent {current_agent.name} (turn {current_turn})",<br>                    )<br>                    if current_turn == 1:<br>                        input_guardrail_results, turn_result = await asyncio.gather(<br>                            cls._run_input_guardrails(<br>                                starting_agent,<br>                                starting_agent.input_guardrails<br>                                + (run_config.input_guardrails or []),<br>                                copy.deepcopy(input),<br>                                context_wrapper,<br>                            ),<br>                            cls._run_single_turn(<br>                                agent=current_agent,<br>                                original_input=original_input,<br>                                generated_items=generated_items,<br>                                hooks=hooks,<br>                                context_wrapper=context_wrapper,<br>                                run_config=run_config,<br>                                should_run_agent_start_hooks=should_run_agent_start_hooks,<br>                            ),<br>                        )<br>                    else:<br>                        turn_result = await cls._run_single_turn(<br>                            agent=current_agent,<br>                            original_input=original_input,<br>                            generated_items=generated_items,<br>                            hooks=hooks,<br>                            context_wrapper=context_wrapper,<br>                            run_config=run_config,<br>                            should_run_agent_start_hooks=should_run_agent_start_hooks,<br>                        )<br>                    should_run_agent_start_hooks = False<br>                    model_responses.append(turn_result.model_response)<br>                    original_input = turn_result.original_input<br>                    generated_items = turn_result.generated_items<br>                    if isinstance(turn_result.next_step, NextStepFinalOutput):<br>                        output_guardrail_results = await cls._run_output_guardrails(<br>                            current_agent.output_guardrails + (run_config.output_guardrails or []),<br>                            current_agent,<br>                            turn_result.next_step.output,<br>                            context_wrapper,<br>                        )<br>                        return RunResult(<br>                            input=original_input,<br>                            new_items=generated_items,<br>                            raw_responses=model_responses,<br>                            final_output=turn_result.next_step.output,<br>                            _last_agent=current_agent,<br>                            input_guardrail_results=input_guardrail_results,<br>                            output_guardrail_results=output_guardrail_results,<br>                        )<br>                    elif isinstance(turn_result.next_step, NextStepHandoff):<br>                        current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)<br>                        current_span.finish(reset_current=True)<br>                        current_span = None<br>                        should_run_agent_start_hooks = True<br>                    elif isinstance(turn_result.next_step, NextStepRunAgain):<br>                        pass<br>                    else:<br>                        raise AgentsException(<br>                            f"Unknown next step type: {type(turn_result.next_step)}"<br>                        )<br>            finally:<br>                if current_span:<br>                    current_span.finish(reset_current=True)<br>    @classmethod<br>    def run_sync(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        *,<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>    ) -> RunResult:<br>        """Run a workflow synchronously, starting at the given agent. Note that this just wraps the<br>        `run` method, so it will not work if there's already an event loop (e.g. inside an async<br>        function, or in a Jupyter notebook or async context like FastAPI). For those cases, use<br>        the `run` method instead.<br>        The agent will run in a loop until a final output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>        Returns:<br>            A run result containing all the inputs, guardrail results and the output of the last<br>            agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>        """<br>        return asyncio.get_event_loop().run_until_complete(<br>            cls.run(<br>                starting_agent,<br>                input,<br>                context=context,<br>                max_turns=max_turns,<br>                hooks=hooks,<br>                run_config=run_config,<br>            )<br>        )<br>    @classmethod<br>    def run_streamed(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>    ) -> RunResultStreaming:<br>        """Run a workflow starting at the given agent in streaming mode. The returned result object<br>        contains a method you can use to stream semantic events as they are generated.<br>        The agent will run in a loop until a final output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>        Returns:<br>            A result object that contains data about the run, as well as a method to stream events.<br>        """<br>        if hooks is None:<br>            hooks = RunHooks[Any]()<br>        if run_config is None:<br>            run_config = RunConfig()<br>        # If there's already a trace, we don't create a new one. In addition, we can't end the<br>        # trace here, because the actual work is done in `stream_events` and this method ends<br>        # before that.<br>        new_trace = (<br>            None<br>            if get_current_trace()<br>            else trace(<br>                workflow_name=run_config.workflow_name,<br>                trace_id=run_config.trace_id,<br>                group_id=run_config.group_id,<br>                metadata=run_config.trace_metadata,<br>                disabled=run_config.tracing_disabled,<br>            )<br>        )<br>        # Need to start the trace here, because the current trace contextvar is captured at<br>        # asyncio.create_task time<br>        if new_trace:<br>            new_trace.start(mark_as_current=True)<br>        output_schema = cls._get_output_schema(starting_agent)<br>        context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(<br>            context=context  # type: ignore<br>        )<br>        streamed_result = RunResultStreaming(<br>            input=copy.deepcopy(input),<br>            new_items=[],<br>            current_agent=starting_agent,<br>            raw_responses=[],<br>            final_output=None,<br>            is_complete=False,<br>            current_turn=0,<br>            max_turns=max_turns,<br>            input_guardrail_results=[],<br>            output_guardrail_results=[],<br>            _current_agent_output_schema=output_schema,<br>            _trace=new_trace,<br>        )<br>        # Kick off the actual agent loop in the background and return the streamed result object.<br>        streamed_result._run_impl_task = asyncio.create_task(<br>            cls._run_streamed_impl(<br>                starting_input=input,<br>                streamed_result=streamed_result,<br>                starting_agent=starting_agent,<br>                max_turns=max_turns,<br>                hooks=hooks,<br>                context_wrapper=context_wrapper,<br>                run_config=run_config,<br>            )<br>        )<br>        return streamed_result<br>    @classmethod<br>    async def _run_input_guardrails_with_queue(<br>        cls,<br>        agent: Agent[Any],<br>        guardrails: list[InputGuardrail[TContext]],<br>        input: str | list[TResponseInputItem],<br>        context: RunContextWrapper[TContext],<br>        streamed_result: RunResultStreaming,<br>        parent_span: Span[Any],<br>    ):<br>        queue = streamed_result._input_guardrail_queue<br>        # We'll run the guardrails and push them onto the queue as they complete<br>        guardrail_tasks = [<br>            asyncio.create_task(<br>                RunImpl.run_single_input_guardrail(agent, guardrail, input, context)<br>            )<br>            for guardrail in guardrails<br>        ]<br>        guardrail_results = []<br>        try:<br>            for done in asyncio.as_completed(guardrail_tasks):<br>                result = await done<br>                if result.output.tripwire_triggered:<br>                    _error_tracing.attach_error_to_span(<br>                        parent_span,<br>                        SpanError(<br>                            message="Guardrail tripwire triggered",<br>                            data={<br>                                "guardrail": result.guardrail.get_name(),<br>                                "type": "input_guardrail",<br>                            },<br>                        ),<br>                    )<br>                queue.put_nowait(result)<br>                guardrail_results.append(result)<br>        except Exception:<br>            for t in guardrail_tasks:<br>                t.cancel()<br>            raise<br>        streamed_result.input_guardrail_results = guardrail_results<br>    @classmethod<br>    async def _run_streamed_impl(<br>        cls,<br>        starting_input: str | list[TResponseInputItem],<br>        streamed_result: RunResultStreaming,<br>        starting_agent: Agent[TContext],<br>        max_turns: int,<br>        hooks: RunHooks[TContext],<br>        context_wrapper: RunContextWrapper[TContext],<br>        run_config: RunConfig,<br>    ):<br>        current_span: Span[AgentSpanData] | None = None<br>        current_agent = starting_agent<br>        current_turn = 0<br>        should_run_agent_start_hooks = True<br>        streamed_result._event_queue.put_nowait(AgentUpdatedStreamEvent(new_agent=current_agent))<br>        try:<br>            while True:<br>                if streamed_result.is_complete:<br>                    break<br>                # Start an agent span if we don't have one. This span is ended if the current<br>                # agent changes, or if the agent loop ends.<br>                if current_span is None:<br>                    handoff_names = [h.agent_name for h in cls._get_handoffs(current_agent)]<br>                    tool_names = [t.name for t in current_agent.tools]<br>                    if output_schema := cls._get_output_schema(current_agent):<br>                        output_type_name = output_schema.output_type_name()<br>                    else:<br>                        output_type_name = "str"<br>                    current_span = agent_span(<br>                        name=current_agent.name,<br>                        handoffs=handoff_names,<br>                        tools=tool_names,<br>                        output_type=output_type_name,<br>                    )<br>                    current_span.start(mark_as_current=True)<br>                current_turn += 1<br>                streamed_result.current_turn = current_turn<br>                if current_turn > max_turns:<br>                    _error_tracing.attach_error_to_span(<br>                        current_span,<br>                        SpanError(<br>                            message="Max turns exceeded",<br>                            data={"max_turns": max_turns},<br>                        ),<br>                    )<br>                    streamed_result._event_queue.put_nowait(QueueCompleteSentinel())<br>                    break<br>                if current_turn == 1:<br>                    # Run the input guardrails in the background and put the results on the queue<br>                    streamed_result._input_guardrails_task = asyncio.create_task(<br>                        cls._run_input_guardrails_with_queue(<br>                            starting_agent,<br>                            starting_agent.input_guardrails + (run_config.input_guardrails or []),<br>                            copy.deepcopy(ItemHelpers.input_to_new_input_list(starting_input)),<br>                            context_wrapper,<br>                            streamed_result,<br>                            current_span,<br>                        )<br>                    )<br>                try:<br>                    turn_result = await cls._run_single_turn_streamed(<br>                        streamed_result,<br>                        current_agent,<br>                        hooks,<br>                        context_wrapper,<br>                        run_config,<br>                        should_run_agent_start_hooks,<br>                    )<br>                    should_run_agent_start_hooks = False<br>                    streamed_result.raw_responses = streamed_result.raw_responses + [<br>                        turn_result.model_response<br>                    ]<br>                    streamed_result.input = turn_result.original_input<br>                    streamed_result.new_items = turn_result.generated_items<br>                    if isinstance(turn_result.next_step, NextStepHandoff):<br>                        current_agent = turn_result.next_step.new_agent<br>                        current_span.finish(reset_current=True)<br>                        current_span = None<br>                        should_run_agent_start_hooks = True<br>                        streamed_result._event_queue.put_nowait(<br>                            AgentUpdatedStreamEvent(new_agent=current_agent)<br>                        )<br>                    elif isinstance(turn_result.next_step, NextStepFinalOutput):<br>                        streamed_result._output_guardrails_task = asyncio.create_task(<br>                            cls._run_output_guardrails(<br>                                current_agent.output_guardrails<br>                                + (run_config.output_guardrails or []),<br>                                current_agent,<br>                                turn_result.next_step.output,<br>                                context_wrapper,<br>                            )<br>                        )<br>                        try:<br>                            output_guardrail_results = await streamed_result._output_guardrails_task<br>                        except Exception:<br>                            # Exceptions will be checked in the stream_events loop<br>                            output_guardrail_results = []<br>                        streamed_result.output_guardrail_results = output_guardrail_results<br>                        streamed_result.final_output = turn_result.next_step.output<br>                        streamed_result.is_complete = True<br>                        streamed_result._event_queue.put_nowait(QueueCompleteSentinel())<br>                    elif isinstance(turn_result.next_step, NextStepRunAgain):<br>                        pass<br>                except Exception as e:<br>                    if current_span:<br>                        _error_tracing.attach_error_to_span(<br>                            current_span,<br>                            SpanError(<br>                                message="Error in agent run",<br>                                data={"error": str(e)},<br>                            ),<br>                        )<br>                    streamed_result.is_complete = True<br>                    streamed_result._event_queue.put_nowait(QueueCompleteSentinel())<br>                    raise<br>            streamed_result.is_complete = True<br>        finally:<br>            if current_span:<br>                current_span.finish(reset_current=True)<br>    @classmethod<br>    async def _run_single_turn_streamed(<br>        cls,<br>        streamed_result: RunResultStreaming,<br>        agent: Agent[TContext],<br>        hooks: RunHooks[TContext],<br>        context_wrapper: RunContextWrapper[TContext],<br>        run_config: RunConfig,<br>        should_run_agent_start_hooks: bool,<br>    ) -> SingleStepResult:<br>        if should_run_agent_start_hooks:<br>            await asyncio.gather(<br>                hooks.on_agent_start(context_wrapper, agent),<br>                (<br>                    agent.hooks.on_start(context_wrapper, agent)<br>                    if agent.hooks<br>                    else _coro.noop_coroutine()<br>                ),<br>            )<br>        output_schema = cls._get_output_schema(agent)<br>        streamed_result.current_agent = agent<br>        streamed_result._current_agent_output_schema = output_schema<br>        system_prompt = await agent.get_system_prompt(context_wrapper)<br>        handoffs = cls._get_handoffs(agent)<br>        model = cls._get_model(agent, run_config)<br>        model_settings = agent.model_settings.resolve(run_config.model_settings)<br>        final_response: ModelResponse | None = None<br>        input = ItemHelpers.input_to_new_input_list(streamed_result.input)<br>        input.extend([item.to_input_item() for item in streamed_result.new_items])<br>        # 1. Stream the output events<br>        async for event in model.stream_response(<br>            system_prompt,<br>            input,<br>            model_settings,<br>            agent.tools,<br>            output_schema,<br>            handoffs,<br>            get_model_tracing_impl(<br>                run_config.tracing_disabled, run_config.trace_include_sensitive_data<br>            ),<br>        ):<br>            if isinstance(event, ResponseCompletedEvent):<br>                usage = (<br>                    Usage(<br>                        requests=1,<br>                        input_tokens=event.response.usage.input_tokens,<br>                        output_tokens=event.response.usage.output_tokens,<br>                        total_tokens=event.response.usage.total_tokens,<br>                    )<br>                    if event.response.usage<br>                    else Usage()<br>                )<br>                final_response = ModelResponse(<br>                    output=event.response.output,<br>                    usage=usage,<br>                    referenceable_id=event.response.id,<br>                )<br>            streamed_result._event_queue.put_nowait(RawResponsesStreamEvent(data=event))<br>        # 2. At this point, the streaming is complete for this turn of the agent loop.<br>        if not final_response:<br>            raise ModelBehaviorError("Model did not produce a final response!")<br>        # 3. Now, we can process the turn as we do in the non-streaming case<br>        single_step_result = await cls._get_single_step_result_from_response(<br>            agent=agent,<br>            original_input=streamed_result.input,<br>            pre_step_items=streamed_result.new_items,<br>            new_response=final_response,<br>            output_schema=output_schema,<br>            handoffs=handoffs,<br>            hooks=hooks,<br>            context_wrapper=context_wrapper,<br>            run_config=run_config,<br>        )<br>        RunImpl.stream_step_result_to_queue(single_step_result, streamed_result._event_queue)<br>        return single_step_result<br>    @classmethod<br>    async def _run_single_turn(<br>        cls,<br>        *,<br>        agent: Agent[TContext],<br>        original_input: str | list[TResponseInputItem],<br>        generated_items: list[RunItem],<br>        hooks: RunHooks[TContext],<br>        context_wrapper: RunContextWrapper[TContext],<br>        run_config: RunConfig,<br>        should_run_agent_start_hooks: bool,<br>    ) -> SingleStepResult:<br>        # Ensure we run the hooks before anything else<br>        if should_run_agent_start_hooks:<br>            await asyncio.gather(<br>                hooks.on_agent_start(context_wrapper, agent),<br>                (<br>                    agent.hooks.on_start(context_wrapper, agent)<br>                    if agent.hooks<br>                    else _coro.noop_coroutine()<br>                ),<br>            )<br>        system_prompt = await agent.get_system_prompt(context_wrapper)<br>        output_schema = cls._get_output_schema(agent)<br>        handoffs = cls._get_handoffs(agent)<br>        input = ItemHelpers.input_to_new_input_list(original_input)<br>        input.extend([generated_item.to_input_item() for generated_item in generated_items])<br>        new_response = await cls._get_new_response(<br>            agent,<br>            system_prompt,<br>            input,<br>            output_schema,<br>            handoffs,<br>            context_wrapper,<br>            run_config,<br>        )<br>        return await cls._get_single_step_result_from_response(<br>            agent=agent,<br>            original_input=original_input,<br>            pre_step_items=generated_items,<br>            new_response=new_response,<br>            output_schema=output_schema,<br>            handoffs=handoffs,<br>            hooks=hooks,<br>            context_wrapper=context_wrapper,<br>            run_config=run_config,<br>        )<br>    @classmethod<br>    async def _get_single_step_result_from_response(<br>        cls,<br>        *,<br>        agent: Agent[TContext],<br>        original_input: str | list[TResponseInputItem],<br>        pre_step_items: list[RunItem],<br>        new_response: ModelResponse,<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        hooks: RunHooks[TContext],<br>        context_wrapper: RunContextWrapper[TContext],<br>        run_config: RunConfig,<br>    ) -> SingleStepResult:<br>        processed_response = RunImpl.process_model_response(<br>            agent=agent,<br>            response=new_response,<br>            output_schema=output_schema,<br>            handoffs=handoffs,<br>        )<br>        return await RunImpl.execute_tools_and_side_effects(<br>            agent=agent,<br>            original_input=original_input,<br>            pre_step_items=pre_step_items,<br>            new_response=new_response,<br>            processed_response=processed_response,<br>            output_schema=output_schema,<br>            hooks=hooks,<br>            context_wrapper=context_wrapper,<br>            run_config=run_config,<br>        )<br>    @classmethod<br>    async def _run_input_guardrails(<br>        cls,<br>        agent: Agent[Any],<br>        guardrails: list[InputGuardrail[TContext]],<br>        input: str | list[TResponseInputItem],<br>        context: RunContextWrapper[TContext],<br>    ) -> list[InputGuardrailResult]:<br>        if not guardrails:<br>            return []<br>        guardrail_tasks = [<br>            asyncio.create_task(<br>                RunImpl.run_single_input_guardrail(agent, guardrail, input, context)<br>            )<br>            for guardrail in guardrails<br>        ]<br>        guardrail_results = []<br>        for done in asyncio.as_completed(guardrail_tasks):<br>            result = await done<br>            if result.output.tripwire_triggered:<br>                # Cancel all guardrail tasks if a tripwire is triggered.<br>                for t in guardrail_tasks:<br>                    t.cancel()<br>                _error_tracing.attach_error_to_current_span(<br>                    SpanError(<br>                        message="Guardrail tripwire triggered",<br>                        data={"guardrail": result.guardrail.get_name()},<br>                    )<br>                )<br>                raise InputGuardrailTripwireTriggered(result)<br>            else:<br>                guardrail_results.append(result)<br>        return guardrail_results<br>    @classmethod<br>    async def _run_output_guardrails(<br>        cls,<br>        guardrails: list[OutputGuardrail[TContext]],<br>        agent: Agent[TContext],<br>        agent_output: Any,<br>        context: RunContextWrapper[TContext],<br>    ) -> list[OutputGuardrailResult]:<br>        if not guardrails:<br>            return []<br>        guardrail_tasks = [<br>            asyncio.create_task(<br>                RunImpl.run_single_output_guardrail(guardrail, agent, agent_output, context)<br>            )<br>            for guardrail in guardrails<br>        ]<br>        guardrail_results = []<br>        for done in asyncio.as_completed(guardrail_tasks):<br>            result = await done<br>            if result.output.tripwire_triggered:<br>                # Cancel all guardrail tasks if a tripwire is triggered.<br>                for t in guardrail_tasks:<br>                    t.cancel()<br>                _error_tracing.attach_error_to_current_span(<br>                    SpanError(<br>                        message="Guardrail tripwire triggered",<br>                        data={"guardrail": result.guardrail.get_name()},<br>                    )<br>                )<br>                raise OutputGuardrailTripwireTriggered(result)<br>            else:<br>                guardrail_results.append(result)<br>        return guardrail_results<br>    @classmethod<br>    async def _get_new_response(<br>        cls,<br>        agent: Agent[TContext],<br>        system_prompt: str | None,<br>        input: list[TResponseInputItem],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        context_wrapper: RunContextWrapper[TContext],<br>        run_config: RunConfig,<br>    ) -> ModelResponse:<br>        model = cls._get_model(agent, run_config)<br>        model_settings = agent.model_settings.resolve(run_config.model_settings)<br>        new_response = await model.get_response(<br>            system_instructions=system_prompt,<br>            input=input,<br>            model_settings=model_settings,<br>            tools=agent.tools,<br>            output_schema=output_schema,<br>            handoffs=handoffs,<br>            tracing=get_model_tracing_impl(<br>                run_config.tracing_disabled, run_config.trace_include_sensitive_data<br>            ),<br>        )<br>        context_wrapper.usage.add(new_response.usage)<br>        return new_response<br>    @classmethod<br>    def _get_output_schema(cls, agent: Agent[Any]) -> AgentOutputSchema | None:<br>        if agent.output_type is None or agent.output_type is str:<br>            return None<br>        return AgentOutputSchema(agent.output_type)<br>    @classmethod<br>    def _get_handoffs(cls, agent: Agent[Any]) -> list[Handoff]:<br>        handoffs = []<br>        for handoff_item in agent.handoffs:<br>            if isinstance(handoff_item, Handoff):<br>                handoffs.append(handoff_item)<br>            elif isinstance(handoff_item, Agent):<br>                handoffs.append(handoff(handoff_item))<br>        return handoffs<br>    @classmethod<br>    def _get_model(cls, agent: Agent[Any], run_config: RunConfig) -> Model:<br>        if isinstance(run_config.model, Model):<br>            return run_config.model<br>        elif isinstance(run_config.model, str):<br>            return run_config.model_provider.get_model(run_config.model)<br>        elif isinstance(agent.model, Model):<br>            return agent.model<br>        return run_config.model_provider.get_model(agent.model)<br>``` |

#### run`async``classmethod`

```md-code__content
run(
    starting_agent: Agent[TContext],
    input: str | list[TResponseInputItem],
    *,
    context: TContext | None = None,
    max_turns: int = DEFAULT_MAX_TURNS,
    hooks: RunHooks[TContext] | None = None,
    run_config: RunConfig | None = None,
) -> RunResult

```

Run a workflow starting at the given agent. The agent will run in a loop until a final
output is generated. The loop runs like so:
1\. The agent is invoked with the given input.
2\. If there is a final output (i.e. the agent produces something of type
`agent.output_type`, the loop terminates.
3\. If there's a handoff, we run the loop again, with the new agent.
4\. Else, we run tool calls (if any), and re-run the loop.

In two cases, the agent may raise an exception:
1\. If the max\_turns is exceeded, a MaxTurnsExceeded exception is raised.
2\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.

Note that only the first agent's input guardrails are run.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `starting_agent` | `Agent[TContext]` | The starting agent to run. | _required_ |
| `input` | `str | list[TResponseInputItem]` | The initial input to the agent. You can pass a single string for a user message,<br>or a list of input items. | _required_ |
| `context` | `TContext | None` | The context to run the agent with. | `None` |
| `max_turns` | `int` | The maximum number of turns to run the agent for. A turn is defined as one<br>AI invocation (including any tool calls that might occur). | `DEFAULT_MAX_TURNS` |
| `hooks` | `RunHooks[TContext] | None` | An object that receives callbacks on various lifecycle events. | `None` |
| `run_config` | `RunConfig | None` | Global settings for the entire agent run. | `None` |

Returns:

| Type | Description |
| --- | --- |
| `RunResult` | A run result containing all the inputs, guardrail results and the output of the last |
| `RunResult` | agent. Agents may perform handoffs, so we don't know the specific type of the output. |

Source code in `src/agents/run.py`

|     |     |
| --- | --- |
| ```<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>``` | ```md-code__content<br>@classmethod<br>async def run(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    *,<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>) -> RunResult:<br>    """Run a workflow starting at the given agent. The agent will run in a loop until a final<br>    output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>    Returns:<br>        A run result containing all the inputs, guardrail results and the output of the last<br>        agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>    """<br>    if hooks is None:<br>        hooks = RunHooks[Any]()<br>    if run_config is None:<br>        run_config = RunConfig()<br>    with TraceCtxManager(<br>        workflow_name=run_config.workflow_name,<br>        trace_id=run_config.trace_id,<br>        group_id=run_config.group_id,<br>        metadata=run_config.trace_metadata,<br>        disabled=run_config.tracing_disabled,<br>    ):<br>        current_turn = 0<br>        original_input: str | list[TResponseInputItem] = copy.deepcopy(input)<br>        generated_items: list[RunItem] = []<br>        model_responses: list[ModelResponse] = []<br>        context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(<br>            context=context,  # type: ignore<br>        )<br>        input_guardrail_results: list[InputGuardrailResult] = []<br>        current_span: Span[AgentSpanData] | None = None<br>        current_agent = starting_agent<br>        should_run_agent_start_hooks = True<br>        try:<br>            while True:<br>                # Start an agent span if we don't have one. This span is ended if the current<br>                # agent changes, or if the agent loop ends.<br>                if current_span is None:<br>                    handoff_names = [h.agent_name for h in cls._get_handoffs(current_agent)]<br>                    tool_names = [t.name for t in current_agent.tools]<br>                    if output_schema := cls._get_output_schema(current_agent):<br>                        output_type_name = output_schema.output_type_name()<br>                    else:<br>                        output_type_name = "str"<br>                    current_span = agent_span(<br>                        name=current_agent.name,<br>                        handoffs=handoff_names,<br>                        tools=tool_names,<br>                        output_type=output_type_name,<br>                    )<br>                    current_span.start(mark_as_current=True)<br>                current_turn += 1<br>                if current_turn > max_turns:<br>                    _error_tracing.attach_error_to_span(<br>                        current_span,<br>                        SpanError(<br>                            message="Max turns exceeded",<br>                            data={"max_turns": max_turns},<br>                        ),<br>                    )<br>                    raise MaxTurnsExceeded(f"Max turns ({max_turns}) exceeded")<br>                logger.debug(<br>                    f"Running agent {current_agent.name} (turn {current_turn})",<br>                )<br>                if current_turn == 1:<br>                    input_guardrail_results, turn_result = await asyncio.gather(<br>                        cls._run_input_guardrails(<br>                            starting_agent,<br>                            starting_agent.input_guardrails<br>                            + (run_config.input_guardrails or []),<br>                            copy.deepcopy(input),<br>                            context_wrapper,<br>                        ),<br>                        cls._run_single_turn(<br>                            agent=current_agent,<br>                            original_input=original_input,<br>                            generated_items=generated_items,<br>                            hooks=hooks,<br>                            context_wrapper=context_wrapper,<br>                            run_config=run_config,<br>                            should_run_agent_start_hooks=should_run_agent_start_hooks,<br>                        ),<br>                    )<br>                else:<br>                    turn_result = await cls._run_single_turn(<br>                        agent=current_agent,<br>                        original_input=original_input,<br>                        generated_items=generated_items,<br>                        hooks=hooks,<br>                        context_wrapper=context_wrapper,<br>                        run_config=run_config,<br>                        should_run_agent_start_hooks=should_run_agent_start_hooks,<br>                    )<br>                should_run_agent_start_hooks = False<br>                model_responses.append(turn_result.model_response)<br>                original_input = turn_result.original_input<br>                generated_items = turn_result.generated_items<br>                if isinstance(turn_result.next_step, NextStepFinalOutput):<br>                    output_guardrail_results = await cls._run_output_guardrails(<br>                        current_agent.output_guardrails + (run_config.output_guardrails or []),<br>                        current_agent,<br>                        turn_result.next_step.output,<br>                        context_wrapper,<br>                    )<br>                    return RunResult(<br>                        input=original_input,<br>                        new_items=generated_items,<br>                        raw_responses=model_responses,<br>                        final_output=turn_result.next_step.output,<br>                        _last_agent=current_agent,<br>                        input_guardrail_results=input_guardrail_results,<br>                        output_guardrail_results=output_guardrail_results,<br>                    )<br>                elif isinstance(turn_result.next_step, NextStepHandoff):<br>                    current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)<br>                    current_span.finish(reset_current=True)<br>                    current_span = None<br>                    should_run_agent_start_hooks = True<br>                elif isinstance(turn_result.next_step, NextStepRunAgain):<br>                    pass<br>                else:<br>                    raise AgentsException(<br>                        f"Unknown next step type: {type(turn_result.next_step)}"<br>                    )<br>        finally:<br>            if current_span:<br>                current_span.finish(reset_current=True)<br>``` |

#### run\_sync`classmethod`

```md-code__content
run_sync(
    starting_agent: Agent[TContext],
    input: str | list[TResponseInputItem],
    *,
    context: TContext | None = None,
    max_turns: int = DEFAULT_MAX_TURNS,
    hooks: RunHooks[TContext] | None = None,
    run_config: RunConfig | None = None,
) -> RunResult

```

Run a workflow synchronously, starting at the given agent. Note that this just wraps the
`run` method, so it will not work if there's already an event loop (e.g. inside an async
function, or in a Jupyter notebook or async context like FastAPI). For those cases, use
the `run` method instead.

The agent will run in a loop until a final output is generated. The loop runs like so:
1\. The agent is invoked with the given input.
2\. If there is a final output (i.e. the agent produces something of type
`agent.output_type`, the loop terminates.
3\. If there's a handoff, we run the loop again, with the new agent.
4\. Else, we run tool calls (if any), and re-run the loop.

In two cases, the agent may raise an exception:
1\. If the max\_turns is exceeded, a MaxTurnsExceeded exception is raised.
2\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.

Note that only the first agent's input guardrails are run.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `starting_agent` | `Agent[TContext]` | The starting agent to run. | _required_ |
| `input` | `str | list[TResponseInputItem]` | The initial input to the agent. You can pass a single string for a user message,<br>or a list of input items. | _required_ |
| `context` | `TContext | None` | The context to run the agent with. | `None` |
| `max_turns` | `int` | The maximum number of turns to run the agent for. A turn is defined as one<br>AI invocation (including any tool calls that might occur). | `DEFAULT_MAX_TURNS` |
| `hooks` | `RunHooks[TContext] | None` | An object that receives callbacks on various lifecycle events. | `None` |
| `run_config` | `RunConfig | None` | Global settings for the entire agent run. | `None` |

Returns:

| Type | Description |
| --- | --- |
| `RunResult` | A run result containing all the inputs, guardrail results and the output of the last |
| `RunResult` | agent. Agents may perform handoffs, so we don't know the specific type of the output. |

Source code in `src/agents/run.py`

|     |     |
| --- | --- |
| ```<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>``` | ```md-code__content<br>@classmethod<br>def run_sync(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    *,<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>) -> RunResult:<br>    """Run a workflow synchronously, starting at the given agent. Note that this just wraps the<br>    `run` method, so it will not work if there's already an event loop (e.g. inside an async<br>    function, or in a Jupyter notebook or async context like FastAPI). For those cases, use<br>    the `run` method instead.<br>    The agent will run in a loop until a final output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>    Returns:<br>        A run result containing all the inputs, guardrail results and the output of the last<br>        agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>    """<br>    return asyncio.get_event_loop().run_until_complete(<br>        cls.run(<br>            starting_agent,<br>            input,<br>            context=context,<br>            max_turns=max_turns,<br>            hooks=hooks,<br>            run_config=run_config,<br>        )<br>    )<br>``` |

#### run\_streamed`classmethod`

```md-code__content
run_streamed(
    starting_agent: Agent[TContext],
    input: str | list[TResponseInputItem],
    context: TContext | None = None,
    max_turns: int = DEFAULT_MAX_TURNS,
    hooks: RunHooks[TContext] | None = None,
    run_config: RunConfig | None = None,
) -> RunResultStreaming

```

Run a workflow starting at the given agent in streaming mode. The returned result object
contains a method you can use to stream semantic events as they are generated.

The agent will run in a loop until a final output is generated. The loop runs like so:
1\. The agent is invoked with the given input.
2\. If there is a final output (i.e. the agent produces something of type
`agent.output_type`, the loop terminates.
3\. If there's a handoff, we run the loop again, with the new agent.
4\. Else, we run tool calls (if any), and re-run the loop.

In two cases, the agent may raise an exception:
1\. If the max\_turns is exceeded, a MaxTurnsExceeded exception is raised.
2\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.

Note that only the first agent's input guardrails are run.

Parameters:

| Name | Type | Description | Default |
| --- | --- | --- | --- |
| `starting_agent` | `Agent[TContext]` | The starting agent to run. | _required_ |
| `input` | `str | list[TResponseInputItem]` | The initial input to the agent. You can pass a single string for a user message,<br>or a list of input items. | _required_ |
| `context` | `TContext | None` | The context to run the agent with. | `None` |
| `max_turns` | `int` | The maximum number of turns to run the agent for. A turn is defined as one<br>AI invocation (including any tool calls that might occur). | `DEFAULT_MAX_TURNS` |
| `hooks` | `RunHooks[TContext] | None` | An object that receives callbacks on various lifecycle events. | `None` |
| `run_config` | `RunConfig | None` | Global settings for the entire agent run. | `None` |

Returns:

| Type | Description |
| --- | --- |
| `RunResultStreaming` | A result object that contains data about the run, as well as a method to stream events. |

Source code in `src/agents/run.py`

|     |     |
| --- | --- |
| ```<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>``` | ```md-code__content<br>@classmethod<br>def run_streamed(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>) -> RunResultStreaming:<br>    """Run a workflow starting at the given agent in streaming mode. The returned result object<br>    contains a method you can use to stream semantic events as they are generated.<br>    The agent will run in a loop until a final output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>    Returns:<br>        A result object that contains data about the run, as well as a method to stream events.<br>    """<br>    if hooks is None:<br>        hooks = RunHooks[Any]()<br>    if run_config is None:<br>        run_config = RunConfig()<br>    # If there's already a trace, we don't create a new one. In addition, we can't end the<br>    # trace here, because the actual work is done in `stream_events` and this method ends<br>    # before that.<br>    new_trace = (<br>        None<br>        if get_current_trace()<br>        else trace(<br>            workflow_name=run_config.workflow_name,<br>            trace_id=run_config.trace_id,<br>            group_id=run_config.group_id,<br>            metadata=run_config.trace_metadata,<br>            disabled=run_config.tracing_disabled,<br>        )<br>    )<br>    # Need to start the trace here, because the current trace contextvar is captured at<br>    # asyncio.create_task time<br>    if new_trace:<br>        new_trace.start(mark_as_current=True)<br>    output_schema = cls._get_output_schema(starting_agent)<br>    context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(<br>        context=context  # type: ignore<br>    )<br>    streamed_result = RunResultStreaming(<br>        input=copy.deepcopy(input),<br>        new_items=[],<br>        current_agent=starting_agent,<br>        raw_responses=[],<br>        final_output=None,<br>        is_complete=False,<br>        current_turn=0,<br>        max_turns=max_turns,<br>        input_guardrail_results=[],<br>        output_guardrail_results=[],<br>        _current_agent_output_schema=output_schema,<br>        _trace=new_trace,<br>    )<br>    # Kick off the actual agent loop in the background and return the streamed result object.<br>    streamed_result._run_impl_task = asyncio.create_task(<br>        cls._run_streamed_impl(<br>            starting_input=input,<br>            streamed_result=streamed_result,<br>            starting_agent=starting_agent,<br>            max_turns=max_turns,<br>            hooks=hooks,<br>            context_wrapper=context_wrapper,<br>            run_config=run_config,<br>        )<br>    )<br>    return streamed_result<br>``` |

### RunConfig`dataclass`

Configures settings for the entire agent run.

Source code in `src/agents/run.py`

|     |     |
| --- | --- |
| ```<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>``` | ```md-code__content<br>@dataclass<br>class RunConfig:<br>    """Configures settings for the entire agent run."""<br>    model: str | Model | None = None<br>    """The model to use for the entire agent run. If set, will override the model set on every<br>    agent. The model_provider passed in below must be able to resolve this model name.<br>    """<br>    model_provider: ModelProvider = field(default_factory=OpenAIProvider)<br>    """The model provider to use when looking up string model names. Defaults to OpenAI."""<br>    model_settings: ModelSettings | None = None<br>    """Configure global model settings. Any non-null values will override the agent-specific model<br>    settings.<br>    """<br>    handoff_input_filter: HandoffInputFilter | None = None<br>    """A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that<br>    will take precedence. The input filter allows you to edit the inputs that are sent to the new<br>    agent. See the documentation in `Handoff.input_filter` for more details.<br>    """<br>    input_guardrails: list[InputGuardrail[Any]] | None = None<br>    """A list of input guardrails to run on the initial run input."""<br>    output_guardrails: list[OutputGuardrail[Any]] | None = None<br>    """A list of output guardrails to run on the final output of the run."""<br>    tracing_disabled: bool = False<br>    """Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.<br>    """<br>    trace_include_sensitive_data: bool = True<br>    """Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or<br>    LLM generations) in traces. If False, we'll still create spans for these events, but the<br>    sensitive data will not be included.<br>    """<br>    workflow_name: str = "Agent workflow"<br>    """The name of the run, used for tracing. Should be a logical name for the run, like<br>    "Code generation workflow" or "Customer support agent".<br>    """<br>    trace_id: str | None = None<br>    """A custom trace ID to use for tracing. If not provided, we will generate a new trace ID."""<br>    group_id: str | None = None<br>    """<br>    A grouping identifier to use for tracing, to link multiple traces from the same conversation<br>    or process. For example, you might use a chat thread ID.<br>    """<br>    trace_metadata: dict[str, Any] | None = None<br>    """<br>    An optional dictionary of additional metadata to include with the trace.<br>    """<br>``` |

#### model`class-attribute``instance-attribute`

```md-code__content
model: str | Model | None = None

```

The model to use for the entire agent run. If set, will override the model set on every
agent. The model\_provider passed in below must be able to resolve this model name.

#### model\_provider`class-attribute``instance-attribute`

```md-code__content
model_provider: ModelProvider = field(
    default_factory=OpenAIProvider
)

```

The model provider to use when looking up string model names. Defaults to OpenAI.

#### model\_settings`class-attribute``instance-attribute`

```md-code__content
model_settings: ModelSettings | None = None

```

Configure global model settings. Any non-null values will override the agent-specific model
settings.

#### handoff\_input\_filter`class-attribute``instance-attribute`

```md-code__content
handoff_input_filter: HandoffInputFilter | None = None

```

A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that
will take precedence. The input filter allows you to edit the inputs that are sent to the new
agent. See the documentation in `Handoff.input_filter` for more details.

#### input\_guardrails`class-attribute``instance-attribute`

```md-code__content
input_guardrails: list[InputGuardrail[Any]] | None = None

```

A list of input guardrails to run on the initial run input.

#### output\_guardrails`class-attribute``instance-attribute`

```md-code__content
output_guardrails: list[OutputGuardrail[Any]] | None = None

```

A list of output guardrails to run on the final output of the run.

#### tracing\_disabled`class-attribute``instance-attribute`

```md-code__content
tracing_disabled: bool = False

```

Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.

#### trace\_include\_sensitive\_data`class-attribute``instance-attribute`

```md-code__content
trace_include_sensitive_data: bool = True

```

Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or
LLM generations) in traces. If False, we'll still create spans for these events, but the
sensitive data will not be included.

#### workflow\_name`class-attribute``instance-attribute`

```md-code__content
workflow_name: str = 'Agent workflow'

```

The name of the run, used for tracing. Should be a logical name for the run, like
"Code generation workflow" or "Customer support agent".

#### trace\_id`class-attribute``instance-attribute`

```md-code__content
trace_id: str | None = None

```

A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.

#### group\_id`class-attribute``instance-attribute`

```md-code__content
group_id: str | None = None

```

A grouping identifier to use for tracing, to link multiple traces from the same conversation
or process. For example, you might use a chat thread ID.

#### trace\_metadata`class-attribute``instance-attribute`

```md-code__content
trace_metadata: dict[str, Any] | None = None

```

An optional dictionary of additional metadata to include with the trace.